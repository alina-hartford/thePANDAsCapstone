{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import configparser\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('aws.cfg')\n",
    "\n",
    "aws_access_key = config['AWS']['aws_access_key_id']\n",
    "aws_secret_key = config['AWS']['aws_secret_access_key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\", aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "\n",
    "# Set the model ID\n",
    "model_id = \"amazon.titan-text-express-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read external landmark data from S3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key\n",
    ")\n",
    "\n",
    "landmarks = s3.download_file('capstone-techcatalyst-conformed', 'group1/landmark_data/part-00000-tid-8270335314995015129-7c07741e-3c51-4b23-9a75-95c6c641450b-65-1-c000.csv', 'part-00000-tid-8270335314995015129-7c07741e-3c51-4b23-9a75-95c6c641450b-65-1-c000.csv')\n",
    "landmark_df = pd.read_csv('/workspaces/thePANDAsCapstone/Pipeline Stages/Databricks/part-00000-tid-8270335314995015129-7c07741e-3c51-4b23-9a75-95c6c641450b-65-1-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary which maps the landmark name to the landmark description to be used with Bedrock\n",
    "landmark_list = landmark_df['Landmark_Name'].to_list()\n",
    "description_list = landmark_df['USE_ORIG'].to_list()\n",
    "build_type = landmark_df['BuildType'].to_list()\n",
    "landmark_dict = {landmark_list[i]: description_list[i] for i in range(len(landmark_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RERUN\n",
    "# Collects all the landmark names categorized\n",
    "categories = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calls Bedrock client by taking in query, running it through the model, and returns the output string\n",
    "\n",
    "def bedrock(query):\n",
    "    user_message = query\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": user_message}],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = client.converse(\n",
    "            modelId=\"amazon.titan-text-express-v1\",\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\":4096,\"stopSequences\":[\"User:\"],\"temperature\":0,\"topP\":1},\n",
    "            additionalModelRequestFields={}\n",
    "        )\n",
    "\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        print(response_text)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that provides the query to the Bedrock function and parses the returned string in JSON format to extract the category of the landmark\n",
    "\n",
    "def categorize(start, end, categories):\n",
    "    print(f'rows {start} - {end}')\n",
    "    query = f\"\"\" {dict(list(landmark_dict.items())[start:end])}\n",
    "\n",
    "    Assign one of each one of these names to one of these categories using the description, choosing from: Architecture, Religion, Transportation, Education, Civic, Entertainment, Financial, Commercial, Industrial, Historic. Leave empty if none of the categories match. Return output in JSON format with keys \"Name\" and \"Category\". Return only one category for every name with nothing in the output except the JSON object.\n",
    "\n",
    "    Use the following descriptions for the categories:\n",
    "    Architecture: infrastructure, structure\n",
    "    Religion: churches, religious\n",
    "    Transportation: airports, railroads, bridges\n",
    "    Education: school, university\n",
    "    Civic: government, military\n",
    "    Entertainment: arts, sports, theaters, museum, recreation, amusement\n",
    "    Financial: banks\n",
    "    Commercial: businesses, shopping, hotel\n",
    "    Industrial: factories\n",
    "    Historic: cemetery, relic\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    response_text = bedrock(query)\n",
    "\n",
    "\n",
    "    pos = response_text.find('{')\n",
    "    pos2 = response_text.find('[')\n",
    "\n",
    "    if response_text[pos] == '{':\n",
    "        json_output = json.loads(response_text[pos:-3])\n",
    "        obj = json_output['rows']\n",
    "    elif response_text[pos2] == '[':\n",
    "        json_output = json.loads(response_text[pos2:-3])\n",
    "        obj = json_output\n",
    "\n",
    "\n",
    "\n",
    "    names = [x['Name'] for x in obj] \n",
    "    category = [x['Category'] for x in obj] \n",
    "\n",
    "    categorized = {names[i]: category[i] for i in range(len(names))}\n",
    "\n",
    "    categories.update(categorized)\n",
    "    return categorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through the landmarks 10 at a time\n",
    "\n",
    "for i in range(10, len(landmark_dict.keys()), 10):    \n",
    "    categorize(i-10, i, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation that all landmarks in original landmark dictionary have been categorizezd\n",
    "diff = landmark_dict.keys() - categories.keys()\n",
    "\n",
    "remaining = {}\n",
    "for i, x in enumerate(list(landmark_dict.keys())):\n",
    "    if x in diff:\n",
    "        remaining[x] = i\n",
    "\n",
    "remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "categories_df = pd.DataFrame({'Name': categories.keys(), 'Category': categories.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cleaning\n",
    "name = categories_df['Name'].drop_duplicates()\n",
    "categories_df['Category'] = categories_df['Category'].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation of categories\n",
    "\n",
    "categories_df['Category'] = categories_df['Category'].replace(to_replace=['Mixed-use','Office Building', 'Hotel'], value='Commercial')\n",
    "categories_df['Category'] = categories_df['Category'].replace(to_replace=['Educational', 'Education'], value='Civic')\n",
    "categories_df['Category'] = categories_df['Category'].replace(to_replace=['Religious'], value='Religion')\n",
    "categories_df['Category'] = categories_df['Category'].replace(to_replace=['Bank'], value='Financial')\n",
    "categories_df['Category'] = categories_df['Category'].replace(to_replace=['Movie Theater', 'Amusement', 'Recreational'], value='Entertainment')\n",
    "categories_df['Category'] = categories_df['Category'].replace(to_replace=['Mausoleum', 'Cemetery'], value='Historic')\n",
    "categories_df['Category'] = categories_df['Category'].replace(to_replace=['Infrastructure'], value='Industrial')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to S3\n",
    "\n",
    "categorized_path = 's3://capstone-techcatalyst-conformed/group1/categorized/categorized_landmarks.csv'\n",
    "categories_df.to_csv(categorized_path, header = 'True', storage_options={\n",
    "                   'key' : aws_access_key,\n",
    "                   'secret' : aws_secret_key\n",
    "               })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
